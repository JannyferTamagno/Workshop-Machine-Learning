{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_uJn-uQZlK7"
      },
      "source": [
        "# Começando o projeto de Machine Learning\n",
        "\n",
        "A primeira etapa ao iniciar um projeto de Machine Learning é definir claramente o problema que queremos resolver.\n",
        "\n",
        "No nosso caso, queremos desenvolver um software que analise exames de câncer de mama e nos informe se o resultado é maligno (câncer) ou benigno (não é câncer).\n",
        "\n",
        "Esse tipo de solução pode auxiliar médicos e especialistas a identificarem rapidamente se um paciente precisa de um tratamento mais imediato ou não.\n",
        "\n",
        "Com uma ferramenta como essa, é possível detectar padrões em exames que nem sempre são evidentes a olho nu, contribuindo para diagnósticos mais precisos e aumentando as chances de tratamento eficaz."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo os dados para o projeto\n",
        "\n",
        "O segundo passo em um projeto de Machine Learning é definir os dados que serão utilizados. No mundo real, a coleta e o tratamento desses dados são etapas essenciais. Muitas vezes, os dados precisam ser armazenados em bancos de dados e organizados de forma a permitir consultas eficientes.\n",
        "\n",
        "Em projetos acadêmicos, é comum utilizar conjuntos de dados disponíveis online, conhecidos como datasets. Se você está começando a estudar Machine Learning, uma ótima fonte para encontrar esses datasets é o site Kaggle, que oferece uma ampla variedade de dados prontos para uso.\n",
        "\n",
        "Antes de iniciar o treinamento do modelo, é importante analisar os dados para entender sua qualidade e adequação. Isso significa que, em situações reais, pode ser necessário buscar mais dados ou até mesmo dados diferentes daqueles que inicialmente possuímos, dependendo do que descobrimos durante a análise.\n",
        "\n",
        "Para este projeto, utilizaremos o banco de dados Wisconsin (Wisconsin Breast Cancer Diagnostic Dataset), que contém informações sobre exames de câncer de mama. Esse dataset é amplamente utilizado em projetos de Machine Learning e está disponível para download [neste link](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic).\n",
        "\n",
        "O banco de dados Wisconsin inclui informações coletadas de exames, onde foram registrados diversos atributos (características) que ajudam a classificar os tumores como malignos ou benignos. Cada registro do dataset corresponde a um exame de mama e possui 30 características diferentes, como textura, forma e tamanho da célula, além do diagnóstico final de cada paciente.\n",
        "\n",
        "Esses dados serão a base para o nosso modelo de Machine Learning, e a primeira etapa será analisá-los para entender melhor seu conteúdo e garantir que possamos utilizá-los de maneira eficaz no desenvolvimento do nosso software de detecção."
      ],
      "metadata": {
        "id": "itsavPLYyfQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando os pacotes necessários\n",
        "\n",
        "Antes de começarmos a analisar os dados, precisamos importar algumas bibliotecas Python que nos ajudarão a manipular e trabalhar com os dados de forma mais eficiente.\n",
        "\n",
        "Mas o que é uma biblioteca? Uma biblioteca é um conjunto de códigos e funções criados por outras pessoas para resolver problemas comuns, economizando tempo e esforço, pois você não precisa reescrever o mesmo código. Você só precisa importar a biblioteca e utilizá-la no seu projeto.\n",
        "\n",
        "Aqui estão as bibliotecas que vamos utilizar e suas funções:\n",
        "\n",
        "* **Pandas:** utilizado para manipulação e análise de dados. Ele permite carregar, transformar e explorar os dados de maneira fácil.\n",
        "* **Matplotlib:** uma biblioteca para criação de gráficos. Vamos utilizá-la para visualizar os dados de forma clara.\n",
        "* **Seaborn:** uma biblioteca baseada no Matplotlib, que torna a visualização de dados ainda mais simples e esteticamente agradável.\n",
        "* **XGBoost:** é um algoritmo de Machine Learning usado para construir modelos preditivos com alto desempenho, neste caso, para classificação de dados.\n",
        "* **Scikit-learn (sklearn):** contém diversas funções úteis, incluindo:\n",
        "* **train_test_split:** para dividir os dados em conjuntos de treinamento e teste, essencial para validar o desempenho do nosso modelo.\n",
        "* **accuracy_score:** uma métrica para calcular a precisão do modelo após a realização das previsões.\n",
        "* **SimpleImputer:** utilizado para preencher valores faltantes nos dados, garantindo que possamos trabalhar com dados completos.\n",
        "\n",
        "\n",
        "Vamos importar as bibliotecas e iniciar o projeto:"
      ],
      "metadata": {
        "id": "KS6dHlC34NrJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Yfo6Xslv1M"
      },
      "source": [
        "# importar as bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lendo os dados\n",
        "Agora que já temos os pacotes necessários importados, vamos começar lendo os nossos dados. Como mencionado antes, utilizaremos o banco de dados Wisconsin, que contém informações sobre exames de câncer de mama. Esse dataset está em formato CSV (Comma-Separated Values), um formato comum para armazenar grandes volumes de dados tabulares.\n",
        "\n",
        "Vamos usar o Pandas para carregar o arquivo de dados. Com a função read_csv() do Pandas, podemos facilmente importar os dados para dentro do nosso projeto e começar a manipulá-los.\n",
        "\n",
        "Aqui está o código para ler o arquivo:"
      ],
      "metadata": {
        "id": "xtXIrBk54loy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fD95uTPl8mw"
      },
      "source": [
        "# importar o dataset em csv\n",
        "url = \"https://www.dropbox.com/s/z8nw6pfumdw3bb9/breast-cancer-wisconsin.csv?raw=1\"\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, estamos:\n",
        "\n",
        "* Definindo a URL do dataset.\n",
        "* Carregando o arquivo CSV e armazenando os dados em um DataFrame chamado df.\n",
        "\n",
        "Esse será o nosso ponto de partida para começar a explorar e analisar os dados. Agora podemos passar para a análise inicial e ver como os dados estão estruturados."
      ],
      "metadata": {
        "id": "jeF1i5CV4QB6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmWnFAV9xcYu"
      },
      "source": [
        "## Análise Exploratória\n",
        "\n",
        "Agora que já importamos nosso dataset, o próximo passo será examinar as dimensões do DataFrame e dar uma olhada nas primeiras entradas dos dados.\n",
        "\n",
        "Isso é importante porque nos ajuda a entender o formato dos dados e sua estrutura geral, permitindo criar uma base para o que faremos a seguir.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AlJd_CM0ljnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Estrutura do Dataset**\n",
        "\n",
        "A coluna `id` representa o número de identificação de cada paciente.\n",
        "\n",
        "A coluna `diagnosis` é a nossa variável alvo, ou seja, o que queremos prever.\n",
        "\n",
        "* M - Indica um diagnóstico maligno (câncer).\n",
        "* B - Indica um diagnóstico benigno (não câncer).\n",
        "\n",
        "Além disso, o dataset contém 30 colunas com medidas numéricas que foram coletadas em exames laboratoriais.\n",
        "\n",
        "Essas medidas ajudam a descrever diferentes características do tumor, como tamanho, forma e textura, e são essenciais para treinar nosso modelo de Machine Learning.\n",
        "\n",
        "Por exemplo:\n",
        "\n",
        "* Raio (radius): Mede o raio médio do tumor.\n",
        "* Textura (texture): Mede a variação na intensidade do tumor na imagem.\n",
        "\n",
        "E várias outras medidas que ajudam a identificar padrões nos dados.\n",
        "Agora, podemos começar a explorar os dados e entender sua distribuição."
      ],
      "metadata": {
        "id": "i5dpzEk_lj2O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJD1uRUzxjrU"
      },
      "source": [
        "# Dimensões do DataFrame\n",
        "print(\"DIMENSÕES DO DATAFRAME:\")\n",
        "print(\"Linhas:\\t\\t{}\".format(df.shape[0]))  # Mostra o número de linhas\n",
        "print(\"Colunas:\\t{}\".format(df.shape[1]))  # Mostra o número de colunas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que o código faz:\n",
        "\n",
        "* `df.shape[0]:` Isso retorna o número de linhas no DataFrame, ou seja, a quantidade de registros ou pacientes.\n",
        "* `df.shape[1]:` Isso retorna o número de colunas, ou seja, quantas variáveis (características) estamos analisando, incluindo a coluna de diagnóstico.\n",
        "\n",
        "Esse código nos mostra de forma clara quantos dados temos disponíveis para analisar, o que é fundamental para termos uma noção da quantidade de informações que nosso modelo de Machine Learning vai utilizar."
      ],
      "metadata": {
        "id": "zfQhsVq96l43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exibindo as primeiras entradas do DataFrame\n",
        "\n",
        "Após verificarmos as dimensões do nosso dataset, o próximo passo é visualizar algumas linhas de dados para entender sua estrutura e conteúdo.\n",
        "\n",
        "Para isso, utilizamos novamente o comando `df.head():`"
      ],
      "metadata": {
        "id": "zxyfwewZ7u5k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zfOZFI0xtu6"
      },
      "source": [
        "# ver as 5 primeiras entradas\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que esse código faz:\n",
        "\n",
        "`df.head():` Exibe as primeiras 5 linhas do DataFrame por padrão.\n",
        "\n",
        "Isso nos permite ver uma amostra dos dados sem carregar o DataFrame inteiro na tela.\n",
        "\n",
        "Esse comando é útil porque dá uma visão rápida de como os dados estão organizados, permitindo identificar colunas, tipos de dados e padrões iniciais. Podemos verificar, por exemplo, como os valores de `ID` e `diagnosis` estão apresentados e ter uma ideia das variáveis numéricas que serão usadas no modelo de Machine Learning."
      ],
      "metadata": {
        "id": "Ih5qU5Oy797U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminando uma coluna com erro e verificando as dimensões\n",
        "\n",
        "Em alguns casos, datasets podem conter colunas desnecessárias ou que foram adicionadas por engano. Aqui, estamos removendo uma coluna chamada `'Unnamed: 32'`, que não traz nenhuma informação útil para o nosso projeto."
      ],
      "metadata": {
        "id": "osAOoa4Y9pRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eliminar uma coluna com erro\n",
        "df.drop('Unnamed: 32', axis=1, inplace=True)\n",
        "\n",
        "# dimensões do df\n",
        "print(\"DIMENSÕES DO DATAFRAME:\")\n",
        "print(\"Linhas:\\t\\t{}\".format(df.shape[0]))\n",
        "print(\"Colunas:\\t{}\".format(df.shape[1]))"
      ],
      "metadata": {
        "id": "ElLHcZas4z1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que esse código faz:\n",
        "* `df.drop():` Remove colunas ou linhas do DataFrame.\n",
        "* `'Unnamed: 32':` O nome da coluna que será removida.\n",
        "* `axis=1:` Especifica que estamos removendo uma coluna. Se fosse axis=0, estaríamos removendo uma linha.\n",
        "* `inplace=True:` Significa que a modificação será feita diretamente no DataFrame original, sem precisar criar uma cópia.\n",
        "\n",
        "Agora que a coluna foi removida, podemos verificar novamente as dimensões do DataFrame para garantir que o número de colunas foi reduzido."
      ],
      "metadata": {
        "id": "UPD4fcdw-s3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ver as 5 primeiras entradas\n",
        "df.head()"
      ],
      "metadata": {
        "id": "S-uZr5205HVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que mudou:\n",
        "\n",
        "* O número de colunas será atualizado, pois removemos uma coluna com erro.\n",
        "\n",
        "Esse processo garante que estamos trabalhando apenas com as colunas que realmente importam para a análise e construção do nosso modelo de Machine Learning."
      ],
      "metadata": {
        "id": "mVCa0tJG_Rli"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9i5OLtt-P43"
      },
      "source": [
        "# Verificando os tipos de dados no DataFrame\n",
        "\n",
        "Após visualizar as primeiras entradas do nosso dataset, percebemos que, com exceção da coluna `diagnosis`, todas as outras colunas parecem ser do tipo numérico (int ou float).\n",
        "\n",
        "No entanto, é importante confirmar isso, pois algumas colunas podem ter sido importadas como strings (texto) por erro ou por conter algum valor inconsistente.\n",
        "\n",
        "Para verificar o tipo de dado de cada coluna, utilizamos o seguinte código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4olnGxiAvUw0"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O que esse código faz:**\n",
        "\n",
        "* `df.dtypes:` Esse comando retorna o tipo de dado de cada coluna do DataFrame.\n",
        " * `int:` Representa números inteiros.\n",
        " * `float:` Representa números com casas decimais.\n",
        " * `object:` Geralmente indica que a coluna contém texto ou strings.\n",
        "\n",
        "**Verificar o tipo de dado é importante porque:**\n",
        "\n",
        "* Precisão: Se uma coluna numérica for interpretada como texto, não poderemos realizar cálculos ou análises estatísticas corretamente.\n",
        "* Tratamento correto: Sabendo o tipo de dado, podemos aplicar as transformações adequadas."
      ],
      "metadata": {
        "id": "gokCMDTgAHuA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aijBwMF-vQo"
      },
      "source": [
        "# Verificando valores ausentes no dataset\n",
        "\n",
        "Em qualquer projeto de Machine Learning, a qualidade dos dados é um dos fatores mais importantes. Dados incompletos ou ausentes podem comprometer a análise e os resultados. Por isso, verificar se existem valores ausentes no dataset é uma etapa essencial antes de qualquer tratamento de dados ou modelagem.\n",
        "\n",
        "Para verificar a presença de valores ausentes, usamos o seguinte código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1b3HY5WvdEb"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O que esse código faz:**\n",
        "* `df.isnull():` Cria uma matriz booleana (com valores True e False), onde True indica a presença de um valor ausente (ou seja, um valor nulo).\n",
        "* `sum():` Soma quantos valores nulos existem em cada coluna, retornando um número para cada uma delas.\n",
        "\n",
        "Esse código nos dá uma visão clara de quantos valores estão faltando em cada coluna do nosso DataFrame.\n",
        "\n",
        "Como o resultado deu 0 em todas as colunas isso significa que não há valores ausentes, e o dataset está completo. Caso contrário, teriamos que decidir como lidar com esses valores ausentes, seja removendo ou preenchendo (imputando) os dados de forma adequada.\n",
        "\n",
        "**Importância da verificação:**\n",
        "* **Impacto no modelo:** Modelos de Machine Learning não funcionam corretamente com dados faltantes. Sem tratamento, os resultados podem ser imprecisos ou falhos.\n",
        "* **Qualidade dos dados:** A quantidade de valores ausentes reflete diretamente na qualidade do dataset. Um dataset com muitos valores ausentes pode exigir um tratamento mais extenso, como preenchimento ou até a remoção de determinadas colunas.\n",
        "\n",
        "**Como lidar com valores ausentes:**\n",
        "* **Remover:** Em alguns casos, é possível simplesmente remover as linhas ou colunas com muitos valores ausentes, especialmente se a quantidade for pequena.\n",
        "* **Preencher (Imputar):** Quando os valores ausentes são importantes, podemos usar técnicas como preencher com a média, mediana, ou valores mais comuns da coluna."
      ],
      "metadata": {
        "id": "u9Yp0gEFBZ82"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLyJr4sH_5B1"
      },
      "source": [
        "# Verificando os valores únicos\n",
        "\n",
        "Mesmo que todas as colunas do nosso conjunto de dados sejam numéricas, é uma boa prática verificar quantos valores diferentes existem em cada coluna. Isso pode nos ajudar a descobrir se alguns números estão sendo usados para representar categorias (como tipos de diagnóstico), em vez de serem apenas números normais.\n",
        "\n",
        "Por exemplo, imagine que temos uma coluna com os números 0 e 1. Esses números podem não ser simples números, mas sim representar algo, como 0 = não tem câncer e 1 = tem câncer. Ou seja, os números podem estar codificando categorias.\n",
        "\n",
        "Ao verificar quantos valores únicos existem em cada coluna, podemos ter uma ideia se os números são contínuos (medidas) ou se representam grupos ou categorias.\n",
        "\n",
        "Aqui está o código que usamos para essa verificação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03UyshwIv8_j"
      },
      "source": [
        "# valores únicos\n",
        "print(\"\\nVALORES ÚNICOS:\")\n",
        "print(df.nunique().sort_values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Por que verificar os valores únicos é importante?**\n",
        "\n",
        "* **Identificar classes:** Se uma coluna tem poucos valores diferentes (por exemplo, 2 ou 3), isso pode significar que ela está representando categorias ou classes. Por exemplo, a coluna diagnosis tem apenas dois valores únicos: M para maligno (câncer) e B para benigno (não câncer).\n",
        "\n",
        "* **Detectar categorias disfarçadas:** Às vezes, números podem estar representando categorias, como 1 para masculino e 2 para feminino. Se tratarmos esses números como se fossem normais (quantidades), isso pode causar problemas na análise.\n",
        "\n",
        "* **Entender a variação:** Saber quantos valores diferentes existem em uma coluna nos ajuda a entender se a coluna varia muito ou pouco. Isso é importante para decidir como usar esses dados no nosso modelo de Machine Learning.\n",
        "\n",
        "**Exemplo:**\n",
        "\n",
        "Se tivermos uma coluna com apenas dois valores únicos, como 0 e 1, isso pode indicar que ela representa uma resposta binária, como \"sim\" e \"não\". Já colunas com muitos valores diferentes, como medidas de tumor, são provavelmente variáveis contínuas (números normais que podem ter muitos valores diferentes)."
      ],
      "metadata": {
        "id": "Ss9sj-24EYrK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MViHBQbZw-wp"
      },
      "source": [
        "# Verificando o balanceamento do dataset\n",
        "Agora, vamos verificar o balanceamento dos dados, ou seja, quantas vezes cada tipo de diagnóstico (maligno ou benigno) aparece no nosso conjunto de dados.\n",
        "\n",
        "Isso é importante porque, se houver muitos mais exemplos de um tipo de diagnóstico do que do outro, pode ser mais difícil para o modelo aprender a reconhecer o tipo menos comum.\n",
        "\n",
        "Vamos começar calculando a porcentagem de cada diagnóstico:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ver porcentagem dos diagnósticos\n",
        "print(\"Diagnósticos:\")\n",
        "print(df.diagnosis.value_counts() / df.shape[0])"
      ],
      "metadata": {
        "id": "UJ4NkeKwEzVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O que esse código faz:**\n",
        "* `df.diagnosis.value_counts():` Conta quantas vezes cada tipo de diagnóstico aparece no dataset.\n",
        "* `/ df.shape[0]:` Divide pela quantidade total de linhas (pacientes) para calcular a porcentagem.\n",
        "\n",
        "Em seguida, vamos criar um gráfico de barras para visualizar essas informações:"
      ],
      "metadata": {
        "id": "W8MEt23uE1u6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1YC709QAXF1"
      },
      "source": [
        "# plotar o gráfico de barras com os diagnósticos\n",
        "fig, ax = plt.subplots()\n",
        "sns.countplot(x='diagnosis', data=df, ax=ax)\n",
        "ax.set_title(\"Diagnósticos\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **O que esse código faz:**\n",
        "\n",
        "* `sns.countplot():` Cria um gráfico de barras que mostra quantos pacientes têm o diagnóstico de maligno (M) e quantos têm o diagnóstico de benigno (B).\n",
        "* `ax.set_title():` Define o título do gráfico, que será \"Diagnósticos\".\n",
        "\n",
        "### **Interpretação:**\n",
        "\n",
        "No gráfico, cada barra representará a quantidade de diagnósticos M (maligno) e B (benigno). Assim, podemos visualizar se há um desbalanceamento entre os dois tipos.\n",
        "\n",
        "Um pequeno desbalanceamento significa que um dos diagnósticos aparece um pouco mais do que o outro, o que pode ser relevante na hora de treinar o nosso modelo de Machine Learning."
      ],
      "metadata": {
        "id": "54YSebmNFFjK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRGrJ04RFwyv"
      },
      "source": [
        "# Visualizando a distribuição das variáveis com histogramas\n",
        "\n",
        "Agora vamos visualizar como as variáveis numéricas estão distribuídas no nosso conjunto de dados. Isso nos ajuda a entender melhor os padrões, como a tendência dos valores se concentrarem em torno de uma média ou se \"puxarem\" mais para um lado do gráfico.\n",
        "\n",
        "Como a coluna `id` não traz informações importantes (é só um identificador), vamos excluí-la ao criar os gráficos.\n",
        "\n",
        "### O que é um histograma?\n",
        "\n",
        "Um histograma é um gráfico de barras que mostra a distribuição dos dados, ou seja, como os valores de uma variável estão espalhados. Ele nos ajuda a entender se os valores se concentram em torno de um valor central (média) ou se são mais \"desviados\" para algum lado.\n",
        "\n",
        "Aqui está o código para plotar os histogramas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nyl-k5fAZTP"
      },
      "source": [
        "# plotar o histograma das features\n",
        "fix, ax = plt.subplots(figsize=(12,8))\n",
        "df.drop('id', axis=1).hist(ax=ax)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O que esse código faz:\n",
        "* `df.drop('id', axis=1):` Remove a coluna id, já que ela não é útil para o nosso gráfico.\n",
        "* `hist():` Cria um histograma para cada variável numérica do dataset, mostrando como os valores estão distribuídos.\n",
        "*` plt.tight_layout():` Ajusta o layout do gráfico para que ele fique bem organizado na tela.\n",
        "\n",
        "### O que observar:\n",
        "Algumas variáveis terão seus valores distribuídos de maneira uniforme ao redor de uma média, formando um gráfico mais simétrico.\n",
        "\n",
        "Outras variáveis podem apresentar valores que se concentram mais à esquerda ou à direita, o que indica que a distribuição dos dados não é equilibrada.\n",
        "Esses gráficos nos ajudam a entender melhor as características dos dados e como cada variável pode influenciar o nosso modelo de Machine Learning."
      ],
      "metadata": {
        "id": "yNcm7rGrF7VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando a correlação das variáveis com Heatmap\n",
        "\n",
        "###**O que é um Heatmap?**\n",
        "\n",
        "Um heatmap é uma representação gráfica onde cores são usadas para mostrar a relação entre diferentes variáveis.\n",
        "\n",
        "Neste caso, estamos utilizando o heatmap para visualizar a correlação entre as variáveis numéricas do nosso dataset, ou seja, o quanto elas estão relacionadas umas com as outras.\n",
        "\n",
        "Correlação é uma medida que mostra como uma variável muda em relação à outra. Ela varia de -1 a 1:\n",
        "\n",
        "* **1:** Significa que duas variáveis estão fortemente correlacionadas positivamente, ou seja, quando uma aumenta, a outra também aumenta.\n",
        "* **-1:** Significa que as variáveis estão fortemente correlacionadas negativamente, ou seja, quando uma aumenta, a outra diminui.\n",
        "* **0:** Significa que não há correlação entre as variáveis, ou seja, elas não influenciam uma à outra.\n"
      ],
      "metadata": {
        "id": "Ruap9XgLGwJI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--HipGyvDsVI"
      },
      "source": [
        "# plotar heatmap\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(df.drop(['id', 'diagnosis'], axis=1).corr(), cmap='coolwarm', ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explicação do código:\n",
        "\n",
        "* `df.drop(['id', 'diagnosis'], axis=1):` Estamos removendo a coluna `id` (que não é importante para essa análise) e a coluna `diagnosis`, pois estamos interessados nas variáveis numéricas.\n",
        "* `.corr():` Esse comando calcula a correlação entre todas as variáveis numéricas do nosso dataset.\n",
        "* `sns.heatmap():` Cria o gráfico de heatmap usando a matriz de correlação gerada pelo `.corr()`. Usamos o esquema de cores 'coolwarm' para representar visualmente os valores de correlação.\n",
        "\n",
        " * As cores mais quentes (vermelhas) indicam correlação positiva alta.\n",
        " * As cores mais frias (azuis) indicam correlação negativa alta.\n",
        "\n",
        "### O que observar no Heatmap:\n",
        "\n",
        "Cores escuras ou vermelhas fortes indicam uma alta correlação positiva. Isso significa que, à medida que uma variável aumenta, a outra também tende a aumentar.\n",
        "\n",
        "Cores escuras ou azuis fortes indicam uma alta correlação negativa. Isso significa que, à medida que uma variável aumenta, a outra tende a diminuir.\n",
        "Cores claras ou próximas do branco indicam que não há muita correlação entre essas variáveis, ou seja, elas não têm uma relação clara.\n",
        "\n",
        "### Por que isso é importante?\n",
        "\n",
        "* **Correlação alta:** Se duas variáveis estão fortemente correlacionadas, pode ser que uma delas não seja necessária no modelo, pois ambas estão fornecendo informações muito parecidas. Isso ajuda a simplificar o modelo.\n",
        "\n",
        "* **Correlação negativa:** Quando a correlação é negativa, significa que as variáveis têm comportamentos opostos, o que também pode ser uma informação importante para o modelo.\n"
      ],
      "metadata": {
        "id": "oedCAjqVHYVX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEySiTKmGihk"
      },
      "source": [
        "## Preparação dos dados\n",
        "\n",
        "Agora que temos nossos dados prontos, precisamos fazer um passo muito importante chamado pré-processamento, que basicamente prepara os dados para que o modelo de Machine Learning consiga trabalhar com eles corretamente.\n",
        "\n",
        "#### **Padronizando os dados com `StandardScaler`**\n",
        "\n",
        "Os valores das variáveis numéricas (como tamanho, textura do tumor, etc.) podem ter escalas diferentes. Por exemplo, algumas variáveis podem ter valores muito pequenos, enquanto outras podem ter valores grandes. Isso pode atrapalhar o modelo, pois ele pode \"dar mais atenção\" para os números maiores, mesmo que eles não sejam mais importantes.\n",
        "\n",
        "Para evitar esse problema, usamos o `StandardScaler`, que ajusta todos os números para ficarem na mesma escala. Assim, o modelo trata todos de maneira justa. O cálculo que ele faz é bem simples:\n",
        "$z = \\frac{x-u}{s}$\n",
        "\n",
        "Aqui:\n",
        "\n",
        "* **$x$** é o valor da variável.\n",
        "* **$u$** é a média dos valores dessa variável no conjunto de treino.\n",
        "* **$s$** é o desvio padrão, que mede o quanto os valores variam em torno da média.\n",
        "\n",
        "Em resumo, o `StandardScaler` transforma os valores para que fiquem padronizados em torno de 0, facilitando o aprendizado do modelo.\n",
        "\n",
        "### **Convertendo a variável alvo com `LabelEncoder`**\n",
        "\n",
        "Nossa variável alvo é o diagnóstico do tumor: M para maligno (câncer) e B para benigno (não é câncer). Esses são valores de texto, e os modelos de Machine Learning funcionam melhor com números. Para resolver isso, usamos o `LabelEncoder` que converte esses diagnósticos em números:\n",
        "\n",
        "* M (maligno) vira 1.\n",
        "* B (benigno) vira 0.\n",
        "\n",
        "### **Dividindo o dataset em treino e teste**\n",
        "\n",
        "Antes de construir o modelo, precisamos dividir os dados em dois grupos:\n",
        "\n",
        "* **Treino:** São os dados que usamos para ensinar o modelo.\n",
        "* **Teste:** São os dados que usamos para verificar se o modelo está funcionando corretamente.\n",
        "\n",
        "Fazemos isso com a função `train_test_split`, que automaticamente divide os dados em 70% para treino e 30% para teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgGGE3-yJIPI"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# separar as variáveis independentes da variável alvo\n",
        "X = df.drop(['diagnosis', 'id'], axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# padronizar as colunas numéricas\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# label encoder na variável alvo\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "# dividir o dataset entre treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHUzXT3OJb_T"
      },
      "source": [
        "### **O que está acontecendo no código:**\n",
        "* **Separando os dados:** Pegamos todas as colunas, exceto diagnosis (que é o que queremos prever) e id (que não é útil para o modelo). Chamamos essas colunas de X (dados de entrada). A coluna diagnosis é separada como y (o que queremos prever).\n",
        "\n",
        "* **Padronizando os dados:** Usamos o `StandardScaler` para garantir que todas as variáveis numéricas fiquem na mesma escala.\n",
        "\n",
        "* **Convertendo a variável alvo:** Usamos o LabelEncoder para transformar M em 1 e B em 0.\n",
        "\n",
        "* **Dividindo os dados:** Usamos o `train_test_split` para dividir o dataset em treino (70%) e teste (30%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KebSxoHp98D"
      },
      "source": [
        "## Modelo de Machine Learning para detecção do câncer de mama\n",
        "\n",
        "Neste problema, queremos que o modelo de Machine Learning seja capaz de classificar corretamente se um tumor é benigno ou maligno. Para isso, o modelo vai analisar várias informações (chamadas de variáveis independentes, ou features), como o tamanho do tumor, sua forma, entre outros dados, para fazer essa previsão.\n",
        "\n",
        "O modelo que vamos usar é o **Random Forest** (Floresta Aleatória), que é uma escolha excelente por ser flexível e funcionar bem mesmo sem muitos ajustes técnicos.\n",
        "\n",
        "### **O que é o Random Forest?**\n",
        "\n",
        "Random Forest, ou \"Floresta Aleatória\", é um algoritmo de Machine Learning que combina vários \"pequenos modelos\", chamados de árvores de decisão.\n",
        "\n",
        "Imagine que cada árvore de decisão toma decisões baseadas em perguntas simples (por exemplo, \"O tumor tem mais de 5 mm? Sim ou não?\"). No final, cada árvore faz sua previsão, e o Random Forest pega a \"opinião\" de várias árvores para tomar uma decisão final.\n",
        "\n",
        "Por isso o nome \"Floresta Aleatória\" – ele cria várias árvores e combina os resultados, aumentando a precisão da previsão.\n",
        "\n",
        "### **Vantagens do Random Forest:**\n",
        "\n",
        "* **Robustez:** Funciona bem com diferentes tipos de dados e sem muitos ajustes técnicos.\n",
        "* **Flexibilidade:** Mesmo que alguns dados estejam faltando ou com erros, o modelo pode se sair bem, porque usa várias árvores para tomar uma decisão final.\n",
        "\n",
        "Agora vamos ver como usamos o modelo Random Forest no código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqBYq6OrqDf1"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instanciando o modelo de Random Forest\n",
        "ml_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy',\n",
        "                                  random_state = 42)\n",
        "\n",
        "# treinando o modelo\n",
        "ml_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EN8H269xW9_"
      },
      "source": [
        "### Explicação do Código:\n",
        "\n",
        "1. `RandomForestClassifier:` Esse comando cria o modelo de Random Forest. Aqui, nós configuramos:\n",
        " * `n_estimators=10:` Significa que estamos criando 10 árvores de decisão na nossa floresta.\n",
        " * `criterion='entropy':` Define como as árvores tomam suas decisões. Nesse caso, usamos o critério de entropia, que é uma maneira de medir a desorganização das informações.\n",
        " * `random_state=42:` Garante que o modelo será sempre o mesmo se rodarmos o código várias vezes (isso é útil para consistência).\n",
        "\n",
        "2. `ml_model.fit(X_train, y_train):` Aqui, estamos treinando o modelo, ou seja, estamos alimentando-o com os dados de treino para que ele aprenda a identificar se um tumor é benigno ou maligno.\n",
        "\n",
        "###Como funciona o Random Forest:\n",
        "O modelo treina criando várias árvores de decisão. Cada árvore faz previsões separadamente.\n",
        "\n",
        "No final, o Random Forest combina os resultados de todas as árvores e decide qual é a classificação final (benigno ou maligno).\n",
        "\n",
        "###Checando o Desempenho\n",
        "\n",
        "Depois de treinar o modelo, vamos testar seu desempenho para ver como ele se sai ao prever o diagnóstico de novos pacientes. Para isso, utilizamos o conjunto de testes, que são dados que o modelo ainda não viu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkN89ocNyb9U"
      },
      "source": [
        "## Desempenho do modelo de detecção de câncer de mama\n",
        "\n",
        "Quando avaliamos o desempenho de um modelo de Machine Learning, especialmente para um problema tão delicado como a detecção de câncer, nem sempre uma alta acurácia (ou precisão geral) significa que o modelo está realmente funcionando bem.\n",
        "\n",
        "### O que é acurácia?\n",
        "\n",
        "Acurácia é a métrica que nos diz quantas vezes o modelo acertou a classificação, em comparação com o total de casos testados. No entanto, em situações críticas como a detecção de câncer, só olhar para a acurácia pode ser arriscado. Isso porque podemos ter um modelo que acerta muitos casos, mas falha em detectar casos positivos de câncer (o que seria muito perigoso).\n",
        "\n",
        "### Por que outras métricas são importantes?\n",
        "\n",
        "Além da acurácia, existem outras métricas que nos ajudam a entender melhor o desempenho do modelo:\n",
        "\n",
        "* Precisão (Precision): Mede quantos dos casos que o modelo classificou como positivos (câncer) realmente são positivos. Ou seja, quantas das previsões de câncer estão corretas.\n",
        "* Recall: Mede quantos dos casos que realmente são positivos (câncer) o modelo conseguiu identificar. Isso é muito importante, porque queremos minimizar os falsos negativos, ou seja, aqueles casos que têm câncer, mas o modelo classificou como não tendo.\n",
        "* F1-Score: É uma média ponderada entre precisão e recall, ajudando a equilibrar essas duas métricas.\n",
        "* Support: Indica quantas ocorrências de cada classe (benigno ou maligno) existiam no conjunto de teste.\n",
        "\n",
        "Essas métricas são importantes porque, ao lidar com câncer, nosso objetivo principal é maximizar o número de verdadeiros positivos (diagnósticos corretos de câncer) e minimizar os falsos negativos (casos de câncer que o modelo não detectou)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h20z7a_sYJm"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# realizar as previsões no dataset de teste\n",
        "y_pred = ml_model.predict(X_test)\n",
        "\n",
        "# ver acurácia geral\n",
        "print('[Acurácia] Random Forest:', accuracy_score(y_test, y_pred))\n",
        "\n",
        "# imprimir o classification report\n",
        "print('\\n[Classification Report] Random Forest')\n",
        "print('')\n",
        "print( classification_report(y_test, y_pred) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**O que esse código faz:**\n",
        "\n",
        "1. Previsões no conjunto de teste: Usamos o modelo treinado para fazer previsões no conjunto de dados de teste (que o modelo nunca viu antes) e armazenamos os resultados em `y_pred`.\n",
        "2. Acurácia: Calculamos e imprimimos a acurácia geral usando a função `accuracy_score`.\n",
        "3. Classification Report: Usamos o `classification_report` para exibir as métricas de precisão, recall, F1-Score e suporte, que nos dão uma visão mais detalhada do desempenho do modelo.\n",
        "\n",
        "###**Interpretação:**\n",
        "* Acurácia: Nos diz quantas vezes o modelo acertou no geral.\n",
        "* Precisão: Queremos que seja alta para minimizar os falsos positivos (diagnósticos errados de câncer).\n",
        "* Recall: Também queremos um valor alto aqui, porque ele mede a capacidade do modelo de identificar corretamente os casos de câncer (minimizando os falsos negativos).\n",
        "* F1-Score: Um equilíbrio entre precisão e recall, que é importante para ver como o modelo se comporta de maneira geral."
      ],
      "metadata": {
        "id": "-N7-JkR2OwZP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vpMdIA3asj"
      },
      "source": [
        "# Verificando a Matriz de Confusão\n",
        "\n",
        "Embora o modelo tenha apresentado ótimos valores nas métricas de desempenho, como mencionamos antes, cada caso é um caso. Às vezes, uma acurácia alta não significa necessariamente que o modelo está funcionando da melhor maneira possível, especialmente quando falamos de detecção de câncer.\n",
        "\n",
        "Por exemplo, imagine que o modelo tem uma acurácia de 99,999% ao detectar corretamente que um paciente não tem câncer, mas apenas 85% de acurácia ao prever que um paciente tem câncer. Nesse caso, o modelo pode deixar de detectar alguns tumores malignos, o que pode ser muito perigoso.\n",
        "\n",
        "###**Falsos positivos e falsos negativos:**\n",
        "\n",
        "* **Falsos positivos:** O modelo diz que a pessoa tem câncer, mas na verdade não tem. Embora isso possa gerar preocupação, é melhor investigar mais a fundo.\n",
        "* **Falsos negativos:** O modelo diz que a pessoa não tem câncer, mas na verdade tem. Esse é o mais preocupante, pois pode atrasar o tratamento.\n",
        "\n",
        "Às vezes, é melhor errar para o lado de mais falsos positivos, o que nos permite investigar mais a fundo, do que errar com falsos negativos, que podem levar a diagnósticos perdidos. Um exemplo parecido é a detecção de fraudes no cartão de crédito, onde preferimos investigar uma transação suspeita, mesmo que no final ela seja legítima.\n",
        "\n",
        "###**Matriz de Confusão**\n",
        "\n",
        "A matriz de confusão nos ajuda a entender melhor os erros que o modelo está cometendo. Ela nos mostra:\n",
        "\n",
        "* Quantos positivos o modelo previu corretamente.\n",
        "* Quantos negativos o modelo previu corretamente.\n",
        "* E onde o modelo errou (falsos positivos e falsos negativos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4gmoM6Ft98s"
      },
      "source": [
        "# plotar a matriz de confusão\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
        "             index=['neg', 'pos'], columns=['pred_neg', 'pred_pos'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**O que a matriz de confusão nos mostra:**\n",
        "* **neg:** Representa os casos negativos, ou seja, pacientes que não têm câncer.\n",
        "* **pos:** Representa os casos positivos, ou seja, pacientes que têm câncer.\n",
        "* **pred_neg:** Quantos o modelo previu como negativos (sem câncer).\n",
        "* **pred_pos:** Quantos o modelo previu como positivos (com câncer).\n",
        "\n",
        "Com essa matriz, podemos ver onde o modelo acertou e onde ele errou:\n",
        "\n",
        "* **Canto superior esquerdo:** Casos corretamente classificados como negativos (não têm câncer).\n",
        "* **Canto inferior direito:** Casos corretamente classificados como positivos (têm câncer).\n",
        "* **Canto superior direito:** Falsos positivos (o modelo disse que a pessoa tem câncer, mas não tem).\n",
        "* **Canto inferior esquerdo:** Falsos negativos (o modelo disse que a pessoa não tem câncer, mas tem).\n",
        "\n",
        "# Interpretação dos resultados:\n",
        "\n",
        "No caso do Random Forest, o modelo conseguiu atingir uma acurácia superior a **95%** e lidou bem com ambas as classes (positivos e negativos). No entanto, a matriz de confusão nos ajuda a enxergar melhor quantos falsos positivos e quantos falsos negativos ocorreram, permitindo uma análise mais detalhada."
      ],
      "metadata": {
        "id": "JNEpeivJQAmS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTjh4gubyax7"
      },
      "source": [
        "## Outubro Rosa e Machine Learning\n",
        "\n",
        "A campanha do Outubro Rosa acontece todos os anos em outubro, mas a conscientização sobre o câncer de mama deve estar presente durante o ano todo. O diagnóstico precoce é fundamental e salva vidas.\n",
        "\n",
        "Com os avanços na Inteligência Artificial e especialmente no Machine Learning, a tecnologia tem ajudado muito os médicos a diagnosticar casos de câncer de mama. Esses algoritmos podem analisar grandes quantidades de dados e identificar padrões que, muitas vezes, são difíceis de ver a olho nu. Isso tem aumentado bastante as chances de sucesso nos tratamentos, especialmente quando o câncer é detectado cedo.\n",
        "\n",
        "No entanto, por mais que os modelos de Machine Learning sejam confiáveis e possam ajudar na detecção, nada substitui a importância do diagnóstico precoce. Fazer exames regularmente e estar atento aos sinais do corpo é a principal ação que qualquer pessoa pode tomar.\n",
        "\n"
      ]
    }
  ]
}